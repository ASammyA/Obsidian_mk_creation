{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from IPython import get_ipython\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_script_directory():\n",
    "    \"\"\"\n",
    "    Returns the ACTUAL directory containing the notebook/script.\n",
    "    Works in:\n",
    "    - VS Code Jupyter notebooks\n",
    "    - Regular Jupyter Notebook/Lab\n",
    "    - Standalone Python scripts\n",
    "    \"\"\"\n",
    "    # If running in Jupyter\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        try:\n",
    "            # 1. First try VS Code's special attribute\n",
    "            shell = get_ipython()\n",
    "            if hasattr(shell, '__vsc_ipynb_file__'):\n",
    "                return str(Path(shell.__vsc_ipynb_file__).parent)\n",
    "            \n",
    "            # 2. Try Jupyter notebook path (modern Jupyter)\n",
    "            from notebook.notebookapp import list_running_servers\n",
    "            servers = list_running_servers()\n",
    "            if servers:\n",
    "                import requests\n",
    "                from urllib.parse import urljoin\n",
    "                kernel_id = Path(get_ipython().config['IPKernelApp']['connection_file']).stem.replace('kernel-', '')\n",
    "                for server in servers:\n",
    "                    sessions = requests.get(urljoin(server['url'], 'api/sessions'), params={'token': server.get('token', '')}).json()\n",
    "                    for session in sessions:\n",
    "                        if session['kernel']['id'] == kernel_id:\n",
    "                            return str(Path(server['notebook_dir']) / Path(session['notebook']['path']).parent)\n",
    "            \n",
    "            # 3. Fallback to current working directory\n",
    "            return str(Path.cwd())\n",
    "        except:\n",
    "            return str(Path.cwd())\n",
    "    \n",
    "    # If running as a Python script\n",
    "    return str(Path(__file__).parent.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to delete all files and folders in path except .obsidian\n",
    "def delete_all_except_obsidian(path):\n",
    "    \"\"\"\n",
    "    Deletes all files and folders in the given path except for the .obsidian folder.\n",
    "    \"\"\"\n",
    "    for item in os.listdir(path):\n",
    "        item_path = os.path.join(path, item)\n",
    "        if os.path.isdir(item_path) and item != '.obsidian':\n",
    "            # Recursively delete the contents of the directory\n",
    "            for root, dirs, files in os.walk(item_path, topdown=False):\n",
    "                for file in files:\n",
    "                    os.remove(os.path.join(root, file))\n",
    "                for dir in dirs:\n",
    "                    os.rmdir(os.path.join(root, dir))\n",
    "            os.rmdir(item_path)  # Remove the now-empty directory\n",
    "        elif os.path.isfile(item_path):\n",
    "            os.remove(item_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(json_path):\n",
    "    with open(json_path, 'r') as file:\n",
    "        json_contents = json.load(file)\n",
    "    return json_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_url_dict(gid_dict, url_template):\n",
    "    url_dict = {}\n",
    "    for sheet, gid in gid_dict.items():\n",
    "        full_url = url_template.replace(\"edit?gid=gid_value#gid=gid_value\", f\"export?format=csv&gid={gid}\")\n",
    "        url_dict[sheet] = full_url\n",
    "    return url_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_master_url_dict(sheets_dict):\n",
    "    master_url_dict={}\n",
    "    for spreadsheet, spreadsheet_dict in sheets_dict.items():\n",
    "        spreadsheet_gid_dict=spreadsheet_dict['sheets']\n",
    "        spreadsheet_url_template=spreadsheet_dict['link_template']\n",
    "        spreadsheet_url_dict=construct_url_dict(spreadsheet_gid_dict, spreadsheet_url_template)\n",
    "        master_url_dict[spreadsheet]=spreadsheet_url_dict\n",
    "    return master_url_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_first_value(row):\n",
    "    first_value = str(row.iloc[0]).strip() if pd.notna(row.iloc[0]) else None\n",
    "    return first_value\n",
    "\n",
    "def first_column_indecies(df):\n",
    "    indecies_list = df.iloc[1:, 0]\n",
    "    return indecies_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_content_dict_from_url(url):\n",
    "    df = pd.read_csv(url)\n",
    "    \n",
    "    # Convert float columns to Int64 where appropriate\n",
    "    for col in df.select_dtypes(include=['float64']):\n",
    "        if (df[col].dropna().apply(float.is_integer).all()):\n",
    "            df[col] = df[col].astype('Int64')\n",
    "    \n",
    "    content_dict = {}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if pd.notna(row.iloc[0]):  # First column as key\n",
    "            index_value = row.iloc[0]\n",
    "            \n",
    "            if index_value not in content_dict:\n",
    "                content_dict[index_value] = {}\n",
    "            \n",
    "            # Process each column\n",
    "            for i, val in enumerate(row.iloc[1:]):\n",
    "                if pd.notna(val):\n",
    "                    original_col = df.columns[i+1]\n",
    "                    base_col = original_col.split('.')[0]  # Remove pandas suffixes\n",
    "                    \n",
    "                    # Find next available column name\n",
    "                    col_name = base_col\n",
    "                    suffix = 1\n",
    "                    while col_name in content_dict[index_value]:\n",
    "                        suffix += 1\n",
    "                        col_name = f\"{base_col}_{suffix}\"\n",
    "                    \n",
    "                    content_dict[index_value][col_name] = val\n",
    "    \n",
    "    return content_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filename(path: str) -> str:\n",
    "    # Split by forward slash and filter out empty strings (this handles multiple slashes)\n",
    "    parts = [part for part in path.split('/') if part]\n",
    "    # Return the last element, if available; otherwise return an empty string.\n",
    "    return parts[-1] if parts else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_directory(path: str) -> str:\n",
    "    # Remove any trailing slashes so that they don't interfere with finding the last meaningful slash.\n",
    "    stripped_path = path.rstrip('/')\n",
    "    # Find the index of the last slash in the stripped path.\n",
    "    last_slash_index = stripped_path.rfind('/')\n",
    "    # If a slash exists, return everything before it. If not, return an empty string.\n",
    "    return stripped_path[:last_slash_index] if last_slash_index != -1 else ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_lines(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove all literal '/n' substrings and leading spaces from each line,\n",
    "    and remove any trailing empty (or whitespace-only) lines from the input string.\n",
    "    \n",
    "    Args:\n",
    "        s (str): The input string.\n",
    "    \n",
    "    Returns:\n",
    "        str: The cleaned string.\n",
    "    \"\"\"\n",
    "    # Remove all literal occurrences of \"/n\"\n",
    "    s = s.replace(\"/n\", \"\")\n",
    "    \n",
    "    # Split the string into lines.\n",
    "    lines = s.splitlines()\n",
    "    \n",
    "    # Remove trailing empty or whitespace-only lines.\n",
    "    while lines and not lines[-1].strip():\n",
    "        lines.pop()\n",
    "    \n",
    "    # Remove leading spaces from each line.\n",
    "    lines = [line.lstrip() for line in lines]\n",
    "    \n",
    "    # Reassemble the string with newline characters.\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_square_brackets(text):\n",
    "    \"\"\"\n",
    "    Escape single square brackets in the text to prevent them from being treated as links in Markdown.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing square brackets.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with escaped square brackets.\n",
    "    \"\"\"\n",
    "    # Escape single square brackets\n",
    "    return re.sub(r'(?<!\\\\)\\[([^\\]]+)\\]', r'\\\\[\\1\\\\]', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sprite_tags_and_brackets(text):\n",
    "    \"\"\"\n",
    "    Remove <sprite name=...> tags and escape square brackets in the text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    # Remove <sprite name=...> tags\n",
    "    text = re.sub(r'<sprite name=[^>]+>', '', text)\n",
    "    # Escape square brackets\n",
    "    return clean_square_brackets(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_filename(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans a filename by:\n",
    "      - Removing disallowed Windows characters: < > : \" / \\\\ | ? *\n",
    "      - Removing any spaces immediately preceding or following those characters.\n",
    "      - Replacing interior occurrences of a disallowed character (with its adjacent spaces) with an underscore.\n",
    "      - Removing disallowed characters at the beginning or end entirely.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): The original filename string.\n",
    "    \n",
    "    Returns:\n",
    "        str: The cleaned filename.\n",
    "    \"\"\"\n",
    "    # Define the set of disallowed characters.\n",
    "    invalid = r'[<>:\"/\\\\|?*]'\n",
    "    \n",
    "    # 1. Remove invalid characters (with any adjacent spaces) from the beginning.\n",
    "    filename = re.sub(r'^\\s*(?:' + invalid + r'\\s*)+', '', filename)\n",
    "    \n",
    "    # 2. Remove invalid characters (with any adjacent spaces) from the end.\n",
    "    filename = re.sub(r'(?:\\s*' + invalid + r')+\\s*$', '', filename)\n",
    "    \n",
    "    # 3. In the interior, replace any invalid character (with optional surrounding spaces)\n",
    "    #    with a single underscore.\n",
    "    filename = re.sub(r'\\s*(' + invalid + r')\\s*', '_', filename)\n",
    "    \n",
    "    return remove_empty_lines(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_first_row_and_column_duplicates(df, detailed=False):\n",
    "    \"\"\"\n",
    "    Enhanced duplicate checker for first column values and all headers.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        detailed (bool): If True, returns duplicate counts\n",
    "        \n",
    "    Returns:\n",
    "        dict: {\n",
    "            'column_duplicates': bool/Series,  # First column values\n",
    "            'header_duplicates': bool/Series,  # All column headers\n",
    "            'exact_header_duplicates': list   # List of duplicate header names\n",
    "        }\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'column_duplicates': False,\n",
    "        'header_duplicates': False,\n",
    "        'exact_header_duplicates': []\n",
    "    }\n",
    "    \n",
    "    # Check first column (values)\n",
    "    first_col = df.iloc[:, 0]\n",
    "    col_duplicates = first_col[first_col.duplicated(keep=False)]\n",
    "    if detailed:\n",
    "        results['column_duplicates'] = col_duplicates.value_counts().sort_values(ascending=False)\n",
    "    else:\n",
    "        results['column_duplicates'] = not col_duplicates.empty\n",
    "    \n",
    "    # Enhanced header check\n",
    "    header_counts = pd.Series(df.columns).value_counts()\n",
    "    dup_headers = header_counts[header_counts > 1]\n",
    "    \n",
    "    if not dup_headers.empty:\n",
    "        results['header_duplicates'] = True\n",
    "        results['exact_header_duplicates'] = dup_headers.index.tolist()\n",
    "        if detailed:\n",
    "            results['header_duplicates'] = dup_headers.sort_values(ascending=False)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_search_keys(search_keys):\n",
    "    \"\"\"\n",
    "    Enhance search keys by:\n",
    "    - Removing leading 'the' or 'a' and adding individual words for keys separated by underscores or colons.\n",
    "    - Adding variations of words ending with 's' by removing the 's'.\n",
    "\n",
    "    Args:\n",
    "        search_keys (list): List of original search keys.\n",
    "\n",
    "    Returns:\n",
    "        list: Enhanced list of search keys.\n",
    "    \"\"\"\n",
    "    enhanced_keys = set()\n",
    "\n",
    "    for key in search_keys:\n",
    "        # Remove leading 'the' or 'a' (case-insensitive) and the separator after it\n",
    "        key = re.sub(r'^(the|a)[_:]+', '', key, flags=re.IGNORECASE).strip()\n",
    "\n",
    "        # Add the cleaned key to the enhanced keys\n",
    "        enhanced_keys.add(key)\n",
    "\n",
    "        # Split the key by both underscores and colons\n",
    "        parts = re.split(r'[_:]', key)\n",
    "        for part in parts:\n",
    "            if part:  # Ensure the part is not empty\n",
    "                enhanced_keys.add(part)\n",
    "\n",
    "                # If the part ends with 's, add the version without 's\n",
    "                if part.endswith(\"'s\"):\n",
    "                    enhanced_keys.add(part[:-2])\n",
    "                # If the part ends with 's' (without the apostrophe), add the version without 's'\n",
    "                elif part.endswith(\"s\"):\n",
    "                    enhanced_keys.add(part[:-1])\n",
    "\n",
    "    return list(enhanced_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_game_content_dict(config_dict, master_url_dict):\n",
    "    \"\"\"\n",
    "    Constructs the game_content_dict using multithreading for faster processing.\n",
    "\n",
    "    Args:\n",
    "        config_dict (dict): Configuration dictionary containing game information.\n",
    "        master_url_dict (dict): Dictionary containing URLs for each sheet.\n",
    "\n",
    "    Returns:\n",
    "        dict: The constructed game_content_dict.\n",
    "    \"\"\"\n",
    "    game_content_dict = {}\n",
    "\n",
    "    def process_sheet(game, sheet, url):\n",
    "        \"\"\"\n",
    "        Processes a single sheet and returns its content.\n",
    "\n",
    "        Args:\n",
    "            game (str): The game name.\n",
    "            sheet (str): The sheet name.\n",
    "            url (str): The URL of the sheet.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (game, sheet, sheet_content_dict)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            entry_dict = initial_content_dict_from_url(url)\n",
    "            sheet_content_dict = {}\n",
    "\n",
    "            for entry, content in entry_dict.items():\n",
    "                # Clean the entry name for use as a key\n",
    "                clean_entry = clean_filename(entry)\n",
    "                empty_lines_removed_entry = remove_empty_lines(entry)\n",
    "\n",
    "                # Step 1: Clean the content values\n",
    "                cleaned_content = {\n",
    "                    key: clean_sprite_tags_and_brackets(str(value)) if isinstance(value, str) else value\n",
    "                    for key, value in content.items()\n",
    "                }\n",
    "\n",
    "                # Step 2: Capture 'AKA', 'aliases', or 'alias' values\n",
    "                aliases = []\n",
    "                for key in ['AKA', 'aliases', 'alias', 'search keys']:\n",
    "                    if key in content:\n",
    "                        # Split values by commas and strip whitespace\n",
    "                        aliases += [val.strip() for val in str(content[key]).split(',') if val.strip()]\n",
    "\n",
    "                # Step 3: Add 'search_keys' key\n",
    "                search_keys = [empty_lines_removed_entry] + aliases\n",
    "                enhanced_keys = enhance_search_keys(search_keys)\n",
    "\n",
    "                # Step 4: Populate sheet_content_dict with the cleaned content\n",
    "                sheet_content_dict[empty_lines_removed_entry] = {\n",
    "                    'title': empty_lines_removed_entry.strip(),  # Strip title\n",
    "                    'link': f'{game}/{sheet}/{clean_entry}'.strip(),  # Strip link\n",
    "                    'content': cleaned_content,\n",
    "                    'search_keys': enhanced_keys,\n",
    "                    'references': [],\n",
    "                }\n",
    "\n",
    "            return game, sheet, sheet_content_dict\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sheet '{sheet}' for game '{game}': {e}\")\n",
    "            return game, sheet, {}\n",
    "\n",
    "    # Use ThreadPoolExecutor to process sheets concurrently\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for game in config_dict['games']:\n",
    "            if game in master_url_dict:\n",
    "                for sheet, url in master_url_dict[game].items():\n",
    "                    futures.append(executor.submit(process_sheet, game, sheet, url))\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(futures):\n",
    "            game, sheet, sheet_content_dict = future.result()\n",
    "            if game not in game_content_dict:\n",
    "                game_content_dict[game] = {}\n",
    "            game_content_dict[game][sheet] = sheet_content_dict\n",
    "\n",
    "    return game_content_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_initial_meta_dict(master_url_dict):\n",
    "    \"\"\"\n",
    "    Constructs the meta_lore_dict using multithreading for faster processing.\n",
    "\n",
    "    Args:\n",
    "        master_url_dict (dict): Dictionary containing URLs for each sheet in the 'Meta Lore' category.\n",
    "\n",
    "    Returns:\n",
    "        dict: The constructed meta_lore_dict.\n",
    "    \"\"\"\n",
    "    meta_lore_dict = {}\n",
    "\n",
    "    def process_meta_sheet(sheet, url):\n",
    "        \"\"\"\n",
    "        Processes a single meta sheet and returns its content.\n",
    "\n",
    "        Args:\n",
    "            sheet (str): The sheet name.\n",
    "            url (str): The URL of the sheet.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (sheet, sheet_meta_dict)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read CSV with no header\n",
    "            df = pd.read_csv(url, header=None)\n",
    "\n",
    "            # Check if DataFrame has at least two rows (header + at least one data row)\n",
    "            if len(df) > 1:\n",
    "                first_row = df.iloc[1]  # Get the first row after the header\n",
    "\n",
    "                # Extract aliases: non-empty values from cells starting with 'AKA' (case-insensitive)\n",
    "                if str(first_row[0]).strip().upper() == 'AKA':\n",
    "                    aliases = [\n",
    "                        str(x).strip() for x in first_row[1:] if pd.notna(x) and str(x).strip()\n",
    "                    ]\n",
    "                    # Exclude the 'AKA' row from the entry list\n",
    "                    entry_list = df.iloc[2:, 0].tolist()  # Start from the second row after the header\n",
    "                else:\n",
    "                    aliases = []\n",
    "                    entry_list = df.iloc[1:, 0].tolist()  # Include all rows after the header\n",
    "            else:\n",
    "                aliases = []\n",
    "                entry_list = []\n",
    "\n",
    "            # Construct the sheet meta dictionary\n",
    "            sheet_meta_dict = {\n",
    "                'title': sheet,\n",
    "                'search_keys': [sheet] + aliases,\n",
    "                'link': f'Meta Lore/{sheet}',\n",
    "                'references': [],\n",
    "            }\n",
    "\n",
    "            for entry in entry_list:\n",
    "                sheet_meta_dict[entry] = {\n",
    "                    'title': entry,\n",
    "                    'link': f'Meta Lore/{sheet}/{entry}',\n",
    "                    'references': [],\n",
    "                }\n",
    "\n",
    "            return sheet, sheet_meta_dict\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing meta sheet '{sheet}': {e}\")\n",
    "            return sheet, {}\n",
    "\n",
    "    # Use ThreadPoolExecutor to process sheets concurrently\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for sheet, url in master_url_dict['Meta Lore'].items():\n",
    "            futures.append(executor.submit(process_meta_sheet, sheet, url))\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(futures):\n",
    "            sheet, sheet_meta_dict = future.result()\n",
    "            meta_lore_dict[sheet] = sheet_meta_dict\n",
    "\n",
    "    return meta_lore_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_keys_in_strings(search_keys, input_strings):\n",
    "    \"\"\"\n",
    "    Checks if any of the search_keys exist as a string or substring in any of the input strings.\n",
    "\n",
    "    Args:\n",
    "        search_keys (list): List of search keys to check.\n",
    "        input_strings (list): List of strings to search within.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if any search key exists as a string or substring in any of the input strings, False otherwise.\n",
    "    \"\"\"\n",
    "    for input_string in input_strings:\n",
    "        for key in search_keys:\n",
    "            if key in input_string:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_and_update_references(dict_1, dict_2):\n",
    "    \"\"\"\n",
    "    Compares search keys and content between two dictionaries and updates their references.\n",
    "\n",
    "    Args:\n",
    "        dict_1 (dict): The first dictionary with 'search_keys', 'content', and 'references'.\n",
    "        dict_2 (dict): The second dictionary with 'search_keys', 'content', and 'references'.\n",
    "\n",
    "    Returns:\n",
    "        None: Updates the 'references' key in both dictionaries in place.\n",
    "    \"\"\"\n",
    "    # Step 1: Extract search keys and convert to strings\n",
    "    search_keys_1 = set(str(key).strip() for key in dict_1.get('search_keys', []))\n",
    "    search_keys_2 = set(str(key).strip() for key in dict_2.get('search_keys', []))\n",
    "\n",
    "    # Step 2: Check for overlap in search keys\n",
    "    search_key_overlap = not search_keys_1.isdisjoint(search_keys_2)\n",
    "\n",
    "    # Step 3: Extract strings to search in and convert to strings\n",
    "    strings_to_search_in_1 = [str(value) for value in dict_1.get('content', {}).values()]\n",
    "    strings_to_search_in_2 = [str(value) for value in dict_2.get('content', {}).values()]\n",
    "\n",
    "    # Step 4: Use search_keys_in_strings to compare content\n",
    "    result_1 = search_keys_in_strings(search_keys_1, strings_to_search_in_2)\n",
    "    result_2 = search_keys_in_strings(search_keys_2, strings_to_search_in_1)\n",
    "\n",
    "    # Step 5: Combine conditions\n",
    "    if search_key_overlap or result_1 or result_2:\n",
    "        # Update references in dict_1\n",
    "        references_1 = set(dict_1.get('references', []))\n",
    "        references_1.add(dict_2.get('link', '').strip())\n",
    "        dict_1['references'] = list(references_1)\n",
    "\n",
    "        # Update references in dict_2\n",
    "        references_2 = set(dict_2.get('references', []))\n",
    "        references_2.add(dict_1.get('link', '').strip())\n",
    "        dict_2['references'] = list(references_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_all_game_sheets(game_content_dict):\n",
    "    \"\"\"\n",
    "    Cross-reference all entry_dicts across all games and sheets in game_content_dict.\n",
    "    \"\"\"\n",
    "    # Collect all entry_dicts from all games and sheets\n",
    "    all_entry_dicts = []\n",
    "    for game, sheets in game_content_dict.items():\n",
    "        for sheet, entries in sheets.items():\n",
    "            all_entry_dicts.extend(entries.values())\n",
    "\n",
    "    print(f\"Total entries to compare: {len(all_entry_dicts)}\")  # Debugging log\n",
    "\n",
    "    # Generate all unique pairs of entry_dicts\n",
    "    entry_pairs = list(combinations(all_entry_dicts, 2))\n",
    "    print(f\"Total pairs to compare: {len(entry_pairs)}\")  # Debugging log\n",
    "\n",
    "    # Process each pair\n",
    "    for pair in entry_pairs:\n",
    "        dict_1, dict_2 = pair\n",
    "\n",
    "        # Debugging: Log the links of the entries being compared\n",
    "        # print(f\"Comparing '{dict_1['link']}' with '{dict_2['link']}'\")\n",
    "\n",
    "        # Call compare_and_update_references\n",
    "        try:\n",
    "            compare_and_update_references(dict_1, dict_2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error comparing '{dict_1['link']}' and '{dict_2['link']}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_markdown(data, vault_path):\n",
    "    \"\"\"\n",
    "    Convert nested dictionary structure to Markdown files.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Nested dictionary in the format:\n",
    "            {game1: {sheet1: {entry1: {'title': title, 'link': link, \n",
    "                    'content': {key1:value1, key2:value2,...}, 'search_keys': [...], 'references': [...]}}}, ...}\n",
    "        vault_path (str): Root directory where files should be saved\n",
    "    \"\"\"\n",
    "    for game_data in data.values():\n",
    "        for sheet_data in game_data.values():\n",
    "            for entry_data in sheet_data.values():\n",
    "                # Get entry details\n",
    "                title = entry_data.get('title', 'Untitled')\n",
    "                link = entry_data.get('link', '')\n",
    "                content = entry_data.get('content', {})\n",
    "                search_keys = entry_data.get('search_keys', [])\n",
    "                references = entry_data.get('references', [])\n",
    "\n",
    "                # Create full file path\n",
    "                file_path = Path(vault_path) / f\"{link}.md\"\n",
    "\n",
    "                # Create parent directories if they don't exist\n",
    "                file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Write markdown content\n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    # Write each content key-value pair\n",
    "                    for key, value in content.items():\n",
    "                        # Handle multiline values by indenting subsequent lines\n",
    "                        if isinstance(value, str) and '\\n' in value:\n",
    "                            value = value.replace('\\n', '\\n  ')\n",
    "                        f.write(f\"**{key}**: {value}\\n\\n\")\n",
    "\n",
    "                    # Write search keys\n",
    "                    if search_keys:\n",
    "                        f.write(f\"**search keys**: {', '.join(search_keys)}\\n\\n\")\n",
    "\n",
    "                    # Write references at the end\n",
    "                    if references:\n",
    "                        f.write(\"\\n## References\\n\")\n",
    "                        for ref in references:\n",
    "                            f.write(f\"- [[{ref}]]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# from threading import Lock\n",
    "\n",
    "# def dict_to_markdown(data, vault_path, batch_size=100):\n",
    "#     \"\"\"\n",
    "#     Convert nested dictionary structure to Markdown files using multithreading with batch processing.\n",
    "\n",
    "#     Args:\n",
    "#         data (dict): Nested dictionary in the format:\n",
    "#             {game1: {sheet1: {entry1: {'title': title, 'link': link, \n",
    "#                     'content': {key1:value1, key2:value2,...}, 'search_keys': [...], 'references': [...]}}}, ...}\n",
    "#         vault_path (str): Root directory where files should be saved.\n",
    "#         batch_size (int): Number of entries to process in each batch.\n",
    "#     \"\"\"\n",
    "#     dir_lock = Lock()  # Lock for synchronizing directory creation\n",
    "\n",
    "#     def process_batch(batch, vault_path):\n",
    "#         \"\"\"\n",
    "#         Process a batch of entries and write them to Markdown files.\n",
    "\n",
    "#         Args:\n",
    "#             batch (list): List of entry dictionaries.\n",
    "#             vault_path (str): Root directory where files should be saved.\n",
    "#         \"\"\"\n",
    "#         for entry_data in batch:\n",
    "#             # Get entry details\n",
    "#             title = entry_data.get('title', 'Untitled')\n",
    "#             link = entry_data.get('link', '')\n",
    "#             content = entry_data.get('content', {})\n",
    "#             search_keys = entry_data.get('search_keys', [])\n",
    "#             references = entry_data.get('references', [])\n",
    "\n",
    "#             # Create full file path\n",
    "#             file_path = Path(vault_path) / f\"{link}.md\"\n",
    "\n",
    "#             # Ensure parent directories exist\n",
    "#             parent_dir = file_path.parent\n",
    "#             with dir_lock:  # Synchronize directory creation\n",
    "#                 if not parent_dir.exists():\n",
    "#                     parent_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#             # Write markdown content\n",
    "#             with open(file_path, 'w', encoding='utf-8') as f:\n",
    "#                 # Write each content key-value pair\n",
    "#                 for key, value in content.items():\n",
    "#                     # Handle multiline values by indenting subsequent lines\n",
    "#                     if isinstance(value, str) and '\\n' in value:\n",
    "#                         value = value.replace('\\n', '\\n  ')\n",
    "#                     f.write(f\"**{key}**: {value}\\n\\n\")\n",
    "\n",
    "#                 # Write search keys\n",
    "#                 if search_keys:\n",
    "#                     f.write(f\"**search keys**: {', '.join(search_keys)}\\n\\n\")\n",
    "\n",
    "#                 # Write references at the end\n",
    "#                 if references:\n",
    "#                     f.write(\"\\n## References\\n\")\n",
    "#                     for ref in references:\n",
    "#                         f.write(f\"- [[{ref}]]\\n\")\n",
    "\n",
    "#     # Flatten the nested dictionary into a list of entries\n",
    "#     entries = [\n",
    "#         entry_data\n",
    "#         for game_data in data.values()\n",
    "#         for sheet_data in game_data.values()\n",
    "#         for entry_data in sheet_data.values()\n",
    "#     ]\n",
    "\n",
    "#     # Process entries in batches using ThreadPoolExecutor\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         futures = []\n",
    "#         for i in range(0, len(entries), batch_size):\n",
    "#             batch = entries[i:i + batch_size]\n",
    "#             futures.append(executor.submit(process_batch, batch, vault_path))\n",
    "\n",
    "#         # Wait for all threads to complete\n",
    "#         for future in futures:\n",
    "#             future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "script_path = Path(get_script_directory())\n",
    "master_path = script_path.parent\n",
    "vault_path = master_path / 'Obsidian Vault'\n",
    "\n",
    "delete_all_except_obsidian(vault_path)\n",
    "\n",
    "config_path=script_path / 'config.json'\n",
    "sheets_path=script_path / 'sheets.json'\n",
    "\n",
    "# config_dict=load_json(config_path)\n",
    "# sheets_dict=load_json(sheets_path)\n",
    "\n",
    "# master_url_dict=construct_master_url_dict(sheets_dict)\n",
    "\n",
    "# game_content_dict=construct_game_content_dict(config_dict, master_url_dict)\n",
    "# meta_lore_dict=construct_initial_meta_dict(master_url_dict)\n",
    "\n",
    "# dict_to_markdown(game_content_dict, vault_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict=load_json(config_path)\n",
    "sheets_dict=load_json(sheets_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_url_dict=construct_master_url_dict(sheets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_content_dict=construct_game_content_dict(config_dict, master_url_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_lore_dict=construct_initial_meta_dict(master_url_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries to compare: 1280\n",
      "Total pairs to compare: 818560\n"
     ]
    }
   ],
   "source": [
    "process_all_game_sheets(game_content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_markdown(game_content_dict, vault_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Mr Peter Agdistis', 'Dr Ibn Al-Adim', 'LT Arthur Moore', 'Ms Azita Bukhara', 'Lalla Chaima', 'MCO Constance Lee', 'Princess  Coquille Amirejibi', 'Sr. Corso Reverte', 'Dagmar von Nagelsburg', 'DI Douglas Moore', 'Mr Ehsan Fekri', 'Father Stanislav Schaller', 'Mr Fraser Strathcoyne', 'Magister Hokobald ', 'Mlle Margot Mtutine', 'Mme Olympe Bechet', 'Dr Arun Peel', 'Dr Serena Blackwood', 'Dr Yvette Southey', 'Mr Zachary Wakefield'])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_content_dict[\"Book of Hours\"][\"Visitors\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Search Keys: ['Forge of Days', 'Forge of Day']\n"
     ]
    }
   ],
   "source": [
    "search_keys = [\"Forge of Days\"]\n",
    "enhanced_keys = enhance_search_keys(search_keys)\n",
    "print(\"Enhanced Search Keys:\", enhanced_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_smaller_game_content_dict(original_dict, max_entries=1000):\n",
    "#     \"\"\"\n",
    "#     Create a smaller version of the game_content_dict for testing.\n",
    "    \n",
    "#     Args:\n",
    "#         original_dict (dict): The original game_content_dict.\n",
    "#         max_entries (int): The maximum number of entries to include in the smaller dictionary.\n",
    "    \n",
    "#     Returns:\n",
    "#         dict: A smaller version of the game_content_dict.\n",
    "#     \"\"\"\n",
    "#     smaller_dict = {}\n",
    "#     included_entries = 0\n",
    "\n",
    "#     # Ensure the required entries are included\n",
    "#     required_entries = [\n",
    "#         (\"Cultist Simulator\", \"Expeditions\", \"Cater & Hero Limited\"),\n",
    "#         (\"Cultist Simulator\", \"Ingredients\", \"Vital Pigment\")\n",
    "#     ]\n",
    "\n",
    "#     # Add required entries first\n",
    "#     for game, sheet, entry in required_entries:\n",
    "#         if game in original_dict and sheet in original_dict[game] and entry in original_dict[game][sheet]:\n",
    "#             if game not in smaller_dict:\n",
    "#                 smaller_dict[game] = {}\n",
    "#             if sheet not in smaller_dict[game]:\n",
    "#                 smaller_dict[game][sheet] = {}\n",
    "#             smaller_dict[game][sheet][entry] = original_dict[game][sheet][entry]\n",
    "#             included_entries += 1\n",
    "\n",
    "#     # Iterate over the original dictionary to add more entries\n",
    "#     for game, sheets in original_dict.items():\n",
    "#         if game not in smaller_dict:\n",
    "#             smaller_dict[game] = {}\n",
    "#         for sheet, entries in sheets.items():\n",
    "#             if sheet not in smaller_dict[game]:\n",
    "#                 smaller_dict[game][sheet] = {}\n",
    "#             for entry, data in entries.items():\n",
    "#                 # Skip if we've already added the required entries or reached the max limit\n",
    "#                 if included_entries >= max_entries:\n",
    "#                     break\n",
    "#                 if entry not in smaller_dict[game][sheet]:\n",
    "#                     smaller_dict[game][sheet][entry] = data\n",
    "#                     included_entries += 1\n",
    "#             if included_entries >= max_entries:\n",
    "#                 break\n",
    "#         if included_entries >= max_entries:\n",
    "#             break\n",
    "\n",
    "#     return smaller_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# smaller_game_content_dict = create_smaller_game_content_dict(game_content_dict, max_entries=10000)\n",
    "# print(json.dumps(smaller_game_content_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_content_dict['Book of Hours']['Assistance']['Barber']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_content_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_1 = game_content_dict['Cultist Simulator']['Expeditions']['Cater & Hero Limited']\n",
    "# dict_2 = game_content_dict['Cultist Simulator']['Ingredients']['Vital Pigment']\n",
    "# compare_and_update_references(dict_1, dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_lore_dict['Principles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.dirname(script_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(game_content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_lore_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_content_dict['Book of Hours']['Books'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def check_index_duplicates(df, detailed=False):\n",
    "    \"\"\"\n",
    "    Check only for duplicates in the first column (indices)\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame\n",
    "        detailed: If True, returns value counts\n",
    "        \n",
    "    Returns:\n",
    "        Series if detailed=True, else bool\n",
    "    \"\"\"\n",
    "    first_col = df.iloc[:, 0]\n",
    "    duplicates = first_col[first_col.duplicated(keep=False)]\n",
    "    \n",
    "    if detailed:\n",
    "        return duplicates.value_counts().sort_values(ascending=False)\n",
    "    return not duplicates.empty\n",
    "\n",
    "# Main processing loop\n",
    "for game in config_dict['games']:\n",
    "    for sheet, url in master_url_dict[game].items():\n",
    "        try:\n",
    "            # Load the DataFrame directly (no need for header check)\n",
    "            df = pd.read_csv(url)\n",
    "            \n",
    "            # Check only for index duplicates\n",
    "            duplicates = check_index_duplicates(df, detailed=True)\n",
    "            \n",
    "            # Print results if duplicates found\n",
    "            if not duplicates.empty:\n",
    "                print(f'\\n[INDICES] Duplicates found in {game} - {sheet}:')\n",
    "                print(duplicates.to_string())\n",
    "                \n",
    "                # Optional: Show the headers for reference\n",
    "                print(\"\\nCurrent headers:\", df.columns.tolist())\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {game} - {sheet}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(master_url_dict['Book of Hours']['Crafting Recipe'])\n",
    "print(\"Actual headers:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_content_dict\n",
    "    # print(game)\n",
    "    # print(master_url_dict[game])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boh_memory_url=master_url_dict['Book of Hours']['Memories']\n",
    "initial_content_dict_from_url(boh_memory_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create output directory relative to script location\n",
    "# script_dir = Path(get_script_directory())\n",
    "# output_dir = script_dir.parent / \"Obsidian/markdown_files\"\n",
    "# output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# def row_value_pairs(row):\n",
    "#     \"\"\"Convert a pandas row to Obsidian-friendly markdown format\"\"\"\n",
    "#     content = []\n",
    "    \n",
    "#     # Get the first column's value (regardless of other columns)\n",
    "#     first_col_name = row.index[0]  # Name of the first column\n",
    "    \n",
    "#     # Add other columns as key-value pairs (skip the first column)\n",
    "#     # content.append(f\"- **Type**: {type}\")\n",
    "#     for col, val in row.items():\n",
    "#         if col == first_col_name:  # Skip the first column (already used as heading)\n",
    "#             content.append(f\"- **Type**: {col}\")\n",
    "#         if pd.notna(val):\n",
    "#             clean_val = str(val).strip().replace('\\r\\n', '\\n').replace('\\n', '<br>')\n",
    "#             content.append(f\"- **{col}**: {clean_val}\")\n",
    "    \n",
    "#     return \"\\n\".join(content)\n",
    "\n",
    "# def md_files_from_df(df):\n",
    "#     # Write each row to a markdown file\n",
    "#     for index, row in df.iterrows():\n",
    "#         # Create safe filename (remove special chars)\n",
    "#         safe_filename = row_first_value(row).replace(':', ' -') + '.md'\n",
    "#         filepath = output_dir / safe_filename\n",
    "#         # print(f'filepath: {filepath}')\n",
    "        \n",
    "#         try:\n",
    "#             with open(filepath, 'w', encoding='utf-8') as f:\n",
    "#                 f.write(row_value_pairs(row))\n",
    "#             print(f\"✓ Created: {filepath.relative_to(output_dir)}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"✗ Error writing {filepath.name}: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
